{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb6d860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... This may take a moment.\n",
      "WARNING:tensorflow:From C:\\Users\\muzam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muzam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muzam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording for 5 seconds... Please speak clearly.\n",
      "Recording complete.\n",
      "\n",
      "Text recognized: 'hello I want to win this'\n",
      "\n",
      "--- Analysis Results ---\n",
      "\n",
      "Method 1: Text-based Analysis (from Hugging Face Model)\n",
      "  Neutral: 0.61\n",
      "  Surprise: 0.27\n",
      "  Sadness: 0.06\n",
      "  Anger: 0.03\n",
      "  Joy: 0.02\n",
      "  Fear: 0.01\n",
      "  Disgust: 0.00\n",
      "\n",
      "Method 2: Audio-based Analysis (from Hugging Face Model)\n",
      "  Happy: 0.13\n",
      "  Neutral: 0.13\n",
      "  Disgust: 0.13\n",
      "  Calm: 0.13\n",
      "  Surprised: 0.13\n",
      "\n",
      "--- Final Combined Outcome ---\n",
      "Final Prediction: Neutral\n",
      "Combined Scores:\n",
      "  Neutral: 0.37\n",
      "  Surprise: 0.27\n",
      "  Happy: 0.13\n",
      "  Calm: 0.13\n",
      "  Surprised: 0.13\n",
      "  Disgust: 0.07\n",
      "  Sadness: 0.06\n",
      "  Anger: 0.03\n",
      "  Joy: 0.02\n",
      "  Fear: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Speech Emotion Detection using Text and Audio Analysis\n",
    "# This script requires the following libraries. Install them using pip:\n",
    "# pip install sounddevice scipy transformers torch SpeechRecognition pyaudio\n",
    "#\n",
    "# Note: PyAudio can be tricky to install. You may need to install a pre-compiled\n",
    "# wheel file for your system from the PyAudio PyPI page. For some Linux systems,\n",
    "# you may also need to install portaudio19-dev: sudo apt-get install portaudio19-dev\n",
    "# On macOS, you might need to run `brew install portaudio` first.\n",
    "\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import speech_recognition as sr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress Hugging Face warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.modeling_utils\")\n",
    "\n",
    "# --- Configuration ---\n",
    "SAMPLE_RATE = 16000  # Sample rate for audio capture\n",
    "DURATION = 5  # Duration of recording in seconds\n",
    "\n",
    "# --- Model Loading ---\n",
    "\n",
    "print(\"Loading models... This may take a moment.\")\n",
    "try:\n",
    "    # 1. Text-based Emotion Model (Hugging Face)\n",
    "    # This model is fine-tuned for emotion classification from text.\n",
    "    text_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "\n",
    "    # 2. Audio-based Emotion Model (Hugging Face)\n",
    "    # This model is fine-tuned for emotion classification from raw audio.\n",
    "    audio_classifier = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    print(\"Please ensure you have an internet connection and the transformers and torch libraries are correctly installed.\")\n",
    "    exit()\n",
    "\n",
    "# --- Audio Capture Function ---\n",
    "def capture_audio(duration=DURATION, samplerate=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Records audio from the microphone for a specified duration.\n",
    "    \n",
    "    Args:\n",
    "        duration (int): The duration of the recording in seconds.\n",
    "        samplerate (int): The sample rate of the recording.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing the recorded audio data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRecording for {duration} seconds... Please speak clearly.\")\n",
    "    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='float32')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording complete.\")\n",
    "    return recording.squeeze()\n",
    "\n",
    "# --- Analysis Functions ---\n",
    "def text_analysis(audio_data):\n",
    "    \"\"\"\n",
    "    Performs emotion detection on the audio by first converting it to text.\n",
    "    \n",
    "    Args:\n",
    "        audio_data (np.ndarray): The recorded audio data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of emotions and their scores from the text model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the audio data to a temporary WAV file for the speech recognizer\n",
    "        temp_wav_path = \"temp_audio.wav\"\n",
    "        scaled_data = np.int16(audio_data / np.max(np.abs(audio_data)) * 32767)\n",
    "        write(temp_wav_path, SAMPLE_RATE, scaled_data)\n",
    "\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(temp_wav_path) as source:\n",
    "            audio = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"\\nText recognized: '{text}'\")\n",
    "\n",
    "            # Get scores from the text-based emotion classifier\n",
    "            text_result = text_classifier(text)\n",
    "            return {item['label']: item['score'] for item in text_result[0]}\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"\\nText-based analysis failed: Could not understand audio. Try speaking more clearly.\")\n",
    "        return {}\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"\\nText-based analysis failed: Could not request results from Google Speech Recognition service; {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during text analysis: {e}\")\n",
    "        return {}\n",
    "\n",
    "def audio_analysis(audio_data):\n",
    "    \"\"\"\n",
    "    Performs emotion detection directly on the audio waveform.\n",
    "    \n",
    "    Args:\n",
    "        audio_data (np.ndarray): The recorded audio data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of emotions and their scores from the audio model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The audio_classifier expects a specific format.\n",
    "        # We need to wrap the numpy array in a dictionary with the 'sampling_rate' key.\n",
    "        audio_input = {\"raw\": audio_data, \"sampling_rate\": SAMPLE_RATE}\n",
    "        \n",
    "        # Get scores from the audio-based emotion classifier\n",
    "        audio_result = audio_classifier(audio_input)\n",
    "        return {item['label']: item['score'] for item in audio_result}\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during audio analysis: {e}\")\n",
    "        return {}\n",
    "\n",
    "def combine_results(text_scores, audio_scores):\n",
    "    \"\"\"\n",
    "    Combines the results from the two analysis methods.\n",
    "    It takes a simple average of the scores for common emotions.\n",
    "    \n",
    "    Args:\n",
    "        text_scores (dict): Scores from the text-based model.\n",
    "        audio_scores (dict): Scores from the audio-based model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the combined scores (dict) and the final predicted emotion (str).\n",
    "    \"\"\"\n",
    "    combined_scores = {}\n",
    "    \n",
    "    # Identify common emotions to average\n",
    "    common_emotions = set(text_scores.keys()).intersection(set(audio_scores.keys()))\n",
    "    \n",
    "    # Simple averaging for common emotions\n",
    "    for emotion in common_emotions:\n",
    "        combined_scores[emotion] = (text_scores.get(emotion, 0) + audio_scores.get(emotion, 0)) / 2\n",
    "        \n",
    "    # Add unique emotions with their original scores\n",
    "    for emotion, score in text_scores.items():\n",
    "        if emotion not in common_emotions:\n",
    "            combined_scores[emotion] = score\n",
    "    \n",
    "    for emotion, score in audio_scores.items():\n",
    "        if emotion not in common_emotions:\n",
    "            combined_scores[emotion] = score\n",
    "\n",
    "    # Determine the most likely emotion\n",
    "    if combined_scores:\n",
    "        final_emotion = max(combined_scores, key=combined_scores.get)\n",
    "    else:\n",
    "        final_emotion = \"Undetermined\"\n",
    "        \n",
    "    return combined_scores, final_emotion\n",
    "\n",
    "# --- Main Function ---\n",
    "def main():\n",
    "    \"\"\"Main function to run the emotion detection process.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Record audio\n",
    "        audio_data = capture_audio()\n",
    "        \n",
    "        # Step 2: Perform text-based analysis\n",
    "        text_scores = text_analysis(audio_data)\n",
    "\n",
    "        # Step 3: Perform audio-based analysis\n",
    "        audio_scores = audio_analysis(audio_data)\n",
    "\n",
    "        # Step 4: Combine and display results\n",
    "        if not text_scores and not audio_scores:\n",
    "            print(\"\\nCould not perform emotion analysis. Please check your microphone and try again.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n--- Analysis Results ---\")\n",
    "        \n",
    "        print(\"\\nMethod 1: Text-based Analysis (from Hugging Face Model)\")\n",
    "        if text_scores:\n",
    "            for emotion, score in sorted(text_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "                print(f\"  {emotion.capitalize()}: {score:.2f}\")\n",
    "        else:\n",
    "            print(\"  Analysis failed.\")\n",
    "        \n",
    "        print(\"\\nMethod 2: Audio-based Analysis (from Hugging Face Model)\")\n",
    "        if audio_scores:\n",
    "            for emotion, score in sorted(audio_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "                print(f\"  {emotion.capitalize()}: {score:.2f}\")\n",
    "        else:\n",
    "            print(\"  Analysis failed.\")\n",
    "        \n",
    "        # Combine the results\n",
    "        combined_scores, final_emotion = combine_results(text_scores, audio_scores)\n",
    "\n",
    "        print(\"\\n--- Final Combined Outcome ---\")\n",
    "        print(\"Final Prediction:\", final_emotion.capitalize())\n",
    "        print(\"Combined Scores:\")\n",
    "        for emotion, score in sorted(combined_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "            print(f\"  {emotion.capitalize()}: {score:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc7064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576086c0-66a7-4bdb-95a0-5fe4ce435393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
